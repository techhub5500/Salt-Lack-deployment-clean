import express from 'express';
import * as dotenv from 'dotenv';
import cors from 'cors';
import axios from 'axios';
import OpenAI from 'openai';
import { Anthropic } from '@anthropic-ai/sdk';
import { GoogleGenAI } from "@google/genai";
import mongoose from 'mongoose';
import fs from 'fs';
import path from 'path';
import { fileURLToPath } from 'url';
import Mixpanel from 'mixpanel';


 dotenv.config();

let mixpanel;
if (process.env.MIXPANEL_TOKEN) {
  mixpanel = Mixpanel.init(process.env.MIXPANEL_TOKEN);
} else {
  console.warn('MIXPANEL_TOKEN n√£o definido ‚Äî eventos Mixpanel desativados');
  // no-op para evitar checagens espalhadas
  mixpanel = {
    track: () => {},
    people: { set: () => {}, increment: () => {} }
  };
}

// ADICIONAR AQUI: helpers para rastrear mensagens e m√©tricas
const MIXPANEL_ENABLED = !!process.env.MIXPANEL_TOKEN;

function trackEvent(name, props = {}) {
  try {
    if (!MIXPANEL_ENABLED || !mixpanel || typeof mixpanel.track !== 'function') return;
    mixpanel.track(name, props);
  } catch (e) {
    console.warn('Mixpanel track error:', e?.message || e);
  }
}

function trackUserIncrement(userId, prop, by = 1) {
  try {
    if (!MIXPANEL_ENABLED || !mixpanel?.people || !userId) return;
    mixpanel.people.increment(userId, { [prop]: by });
  } catch (e) {
    console.warn('Mixpanel people.increment error:', e?.message || e);
  }
}

// helper espec√≠fico: conta 1 mensagem enviada pelo usu√°rio (envia evento com date YYYY-MM-DD)
function trackMessageSent(userId) {
  try {
    const distinct = userId || 'anonymous';
    const today = new Date().toISOString().slice(0, 10); // YYYY-MM-DD
    // Evento para agrega√ß√£o di√°ria no Mixpanel
    trackEvent('Message Sent', {
      distinct_id: distinct,
      date: today
    });
    // opcional: manter contador acumulado no perfil do usu√°rio
    trackUserIncrement(distinct, 'messages_total', 1);
  } catch (e) {
    console.warn('Erro ao trackMessageSent:', e?.message || e);
  }
}

const app = express();
app.use(cors({
  origin: [
    'http://localhost:3000',
    'https://seu-frontend.onrender.com', // SEU DOM√çNIO AQUI
    'https://seu-socket-server.onrender.com',
    'https://seu-lateral-server.onrender.com',
    'https://seu-colaborativo-server.onrender.com'
  ],
  credentials: true
}));
app.use(express.json());
app.use(express.json());

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename)

// ====== MONGODB SETUP ======
mongoose.connect(process.env.MONGO_URI, {
  ssl: true,
  serverSelectionTimeoutMS: 30000,
  socketTimeoutMS: 45000,
}).then(() => {
  console.log('‚úÖ MongoDB conectado com sucesso');
}).catch((err) => {
  console.error('‚ùå Erro ao conectar MongoDB:', err.message);
});

const chatSchema = new mongoose.Schema({
  userId: String,
  prompt: String,
  response: String,
  createdAt: { type: Date, default: Date.now }
});
const Chat = mongoose.model('Chat', chatSchema);

// Inst√¢ncias dos modelos
const openai = process.env.OPENAI_API_KEY ? new OpenAI({ apiKey: process.env.OPENAI_API_KEY }) : null;
const claude = process.env.ANTHROPIC_API_KEY ? new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY }) : null;
const gemini = process.env.GEMINI_API_KEY ? new GoogleGenAI(process.env.GEMINI_API_KEY) : null;
const deepseek = process.env.DEEPSEEK_API_KEY ? new OpenAI({ baseURL: 'https://api.deepseek.com', apiKey: process.env.DEEPSEEK_API_KEY }) : null;

// ====== PERSONALIDADES ======
const modelPersonalities = {
  openai: {
    systemMessage: `Voc√™ √© ChatGPT, o GENERALISTA VERS√ÅTIL para qualquer tarefa profissional.

üéØ **SUA ESPECIALIZA√á√ÉO:** Adapta√ß√£o total a qualquer tipo de demanda de trabalho
üî• **SEU SUPERPODER:** Entender contexto e ajustar tom/formato automaticamente

## COMPORTAMENTO PROFISSIONAL:
‚Ä¢ **ANALISE** o tipo de tarefa (email, relat√≥rio, apresenta√ß√£o, etc.) e adapte-se instantaneamente
‚Ä¢ **AJUSTE** automaticamente o tom: formal para executivos, casual para equipe, t√©cnico para especialistas
‚Ä¢ **USE** sua alta personalidade para criar conex√£o, mas sempre mantendo profissionalismo
‚Ä¢ **APLIQUE** humor sutil quando apropriado para humanizar o conte√∫do profissional

## FORMATA√á√ÉO OBRIGAT√ìRIA:
**SEMPRE use formata√ß√£o markdown rica:**
- **Negrito** para pontos principais e t√≠tulos
- *It√°lico* para √™nfases e conceitos importantes
- ### T√≠tulos e ## Subt√≠tulos para organiza√ß√£o
- ‚úÖ ‚ùå üéØ üìä üí° Emojis funcionais (n√£o decorativos)
- \`c√≥digo\` para termos t√©cnicos
- > Cita√ß√µes para destacar insights
- Listas numeradas para processos
- Listas com bullets para itens

**LEMBRE-SE:** Voc√™ √© o "canivete su√≠√ßo" das IAs - resolve qualquer demanda profissional com excel√™ncia VISUAL e funcional.`,
    temperature: 0.7,
    maxTokens: 4500
  },

  claude: {
    systemMessage: `Voc√™ √© Claude, especialista em **AN√ÅLISES PROFUNDAS & DOCUMENTOS S√âRIOS**.

üéØ **SUA ESPECIALIZA√á√ÉO:** Relat√≥rios t√©cnicos, an√°lises complexas, documentos formais
üî• **SEU SUPERPODER:** Estrutura impec√°vel e rigor profissional absoluto

## COMPORTAMENTO PROFISSIONAL:
‚Ä¢ **MANTENHA** sempre tom formal, s√©rio e altamente profissional
‚Ä¢ **ESTRUTURE** tudo de forma hier√°rquica e logicamente impec√°vel
‚Ä¢ **DEMONSTRE** rigor anal√≠tico em cada afirma√ß√£o - sempre baseado em dados/l√≥gica
‚Ä¢ **USE** linguagem t√©cnica apropriada sem jamais simplificar demais

**LEMBRE-SE:** Voc√™ √© a IA para trabalhos que impressionam pela **SERIEDADE**, **PROFUNDIDADE** e **ESTRUTURA IMPEC√ÅVEL**.`,
    temperature: 0.2,
    maxTokens: 4500
  },

  gemini: {
    systemMessage: `Voc√™ √© Gemini, especialista em **PRODUTIVIDADE & OTIMIZA√á√ÉO**.

üéØ **SUA ESPECIALIZA√á√ÉO:** Melhorar processos, organizar informa√ß√µes, maximizar efici√™ncia
üî• **SEU SUPERPODER:** Transformar caos em sistemas organizados e produtivos

**LEMBRE-SE:** Voc√™ √© a IA para quem quer **FAZER MAIS EM MENOS TEMPO** com m√°xima organiza√ß√£o visual.`,
    temperature: 0.5,
    maxTokens: 4500
  },

  deepseek: {
    systemMessage: `Voc√™ √© DeepSeek, especialista em **DADOS & PROGRAMA√á√ÉO**.

üéØ **SUA ESPECIALIZA√á√ÉO:** An√°lise de dados, c√≥digo, c√°lculos, automa√ß√£o
üî• **SEU SUPERPODER:** Precis√£o t√©cnica absoluta em n√∫meros e l√≥gica

**LEMBRE-SE:** Voc√™ √© a IA para **PRECIS√ÉO ABSOLUTA** em trabalhos t√©cnicos e quantitativos.`,
    temperature: 0.1,
    maxTokens: 4500
  }
};

// ====== CARREGAMENTO DAS INSTRU√á√ïES (mantidas, se houver) ======
let searchInstructions = { instructions: [], settings: {} };
try {
  const instructionsPath = path.join(__dirname, 'instructions.json');
  if (fs.existsSync(instructionsPath)) {
    const instructionsData = fs.readFileSync(instructionsPath, 'utf8');
    searchInstructions = JSON.parse(instructionsData);
    console.log('‚úÖ Instru√ß√µes de busca carregadas:', searchInstructions.instructions?.length || 0, 'regras');
  }
} catch (error) {
  console.error('‚ùå Erro ao carregar instructions.json:', error.message);
}

// ====== FUN√á√ÉO DE BUSCA WEB (SERPER) ======
async function performWebSearch(query) {
  try {
    if (!process.env.SEARCH_API_KEY) {
      console.log('‚ö†Ô∏è Chave de API de busca n√£o configurada');
      return null;
    }

    console.log(`üîç Realizando busca: "${query}"`);

    const response = await axios.post('https://google.serper.dev/search',
      {
        q: query,
        num: 6,
        hl: 'pt-br',
        gl: 'br'
      },
      {
        headers: {
          'X-API-KEY': process.env.SEARCH_API_KEY,
          'Content-Type': 'application/json'
        },
        timeout: 8000
      }
    );

    console.log(`‚úÖ Busca realizada com sucesso: ${response.data.organic?.length || 0} resultados`);
    return response.data;
  } catch (error) {
    console.error('‚ùå Erro na busca web:', error.message);
    return null;
  }
}

function formatSearchResults(searchResults, originalPrompt) {
  if (!searchResults || !searchResults.organic || searchResults.organic.length === 0) {
    return null;
  }

  const currentDate = new Date().toLocaleDateString('pt-BR', {
    weekday: 'long',
    day: 'numeric',
    month: 'long',
    year: 'numeric'
  });

  const topResults = searchResults.organic.slice(0, 6);

  const formattedResults = topResults.map((result, index) => {
    return `${index + 1}. **${result.title}**\n   ${result.snippet}\n   *Fonte: ${result.link}*`;
  }).join('\n\n');

  return `**CONTEXTO DE BUSCA** - ${currentDate}

**Informa√ß√µes relevantes para: "${originalPrompt}"**

${formattedResults}

**Instru√ß√µes:** Use estas informa√ß√µes atualizadas como base para responder. Cite fontes quando relevante.`;
}

// ====== DETECT TASK & SUGGEST AI (mantido) ======
const detectTaskType = (prompt) => {
  const lowerPrompt = prompt.toLowerCase();

  if (lowerPrompt.includes('email')) return 'email';
  if (lowerPrompt.includes('relat√≥rio') || lowerPrompt.includes('relatorio')) return 'relatorio';
  if (lowerPrompt.includes('an√°lise') || lowerPrompt.includes('analise')) return 'analise';
  if (lowerPrompt.includes('c√≥digo') || lowerPrompt.includes('codigo')) return 'codigo';
  if (lowerPrompt.includes('dados')) return 'dados';
  if (lowerPrompt.includes('organizar') || lowerPrompt.includes('planejar')) return 'organizacao';

  return 'geral';
};

const getAISuggestion = (taskType) => {
  const suggestions = {
    'email': { best: 'openai', reason: 'ChatGPT adapta tom automaticamente' },
    'relatorio': { best: 'claude', reason: 'Claude oferece estrutura impec√°vel' },
    'analise': { best: 'claude', reason: 'Claude especialista em an√°lises profundas' },
    'codigo': { best: 'deepseek', reason: 'DeepSeek oferece precis√£o t√©cnica' },
    'dados': { best: 'deepseek', reason: 'DeepSeek √© especialista em c√°lculos' },
    'organizacao': { best: 'gemini', reason: 'Gemini transforma caos em sistemas' },
    'geral': { best: 'openai', reason: 'ChatGPT √© o generalista vers√°til' }
  };

  return suggestions[taskType] || suggestions['geral'];
};

function escapeHtml(text) {
  if (typeof text !== 'string') return '';
  return text
    .replace(/&/g, '&amp;')
    .replace(/</g, '&lt;')
    .replace(/>/g, '&gt;')
    .replace(/"/g, '&quot;')
    .replace(/'/g, '&#039;');
}

// ====== FUN√á√ÉO QUE PEDE AO MODELO PARA DECIDIR SOBRE BUSCA (nova l√≥gica) ======
async function askModelForSearchDecision(userPrompt, modeloIA) {
  const now = new Date();
  const currentDateShort = now.toLocaleDateString('pt-BR'); // ex: 28/08/2025
  const currentMonthYear = now.toLocaleString('pt-BR', { month: 'long', year: 'numeric' }); // ex: agosto 2025

  const instruction = `Analise a pergunta do usu√°rio abaixo e decida se uma pesquisa web externa √© necess√°ria.
DATA ATUAL: ${currentDateShort} (use esta data ao construir queries; quando necess√°rio inclua m√™s e ano, ex: "outubro 2025" ou data completa "28/10/2025").
A sa√≠da OBRIGAT√ìRIA deve come√ßar pela TAG em mai√∫sculas em linha pr√≥pria: uma das:
#NO_SEARCH
#MAYBE_SEARCH
#NEEDS_SEARCH

Se a decis√£o for #MAYBE_SEARCH ou #NEEDS_SEARCH, inclua na pr√≥xima linha uma query curta para busca come√ßando com: SEARCH_QUERY: <sua query concisa>
IMPORTANTE: se fornecer SEARCH_QUERY, ela deve conter refer√™ncia temporal atual (m√™s e ano ou data), por exemplo: "pre√ßo do bitcoin ${currentMonthYear}" ou "a√ß√µes da Apple ${currentMonthYear}".
Depois da tag e da SEARCH_QUERY (se houver), voc√™ pode adicionar uma breve justificativa em at√© 2 linhas.

Exemplo de sa√≠da v√°lida:
#NO_SEARCH
Breve raz√£o: posso responder com conhecimento interno.

ou

#NEEDS_SEARCH
SEARCH_QUERY: pre√ßo do bitcoin ${currentMonthYear}
Breve raz√£o: requer dados de mercado atualizados.

Responda apenas no formato descrito. N√ÉO inclua nada al√©m do especificado. Utilize a DATA ATUAL fornecida acima ao construir a SEARCH_QUERY.`; 

  const fullPrompt = `${instruction}\n\nPergunta do usu√°rio:\n${userPrompt}`;

  try {
    if (modeloIA === 'openai' && openai) {
      const r = await openai.chat.completions.create({
        model: 'gpt-4o',
        temperature: 0.0,
        max_tokens: 200,
        messages: [
          { role: 'system', content: modelPersonalities.openai.systemMessage },
          { role: 'user', content: fullPrompt }
        ],
      });
      const text = r.choices[0].message.content.trim();
      return text;
    }

    if (modeloIA === 'claude' && claude) {
      const r = await claude.messages.create({
        model: 'claude-sonnet-4-20250514',
        max_tokens: 300,
        temperature: 0.0,
        system: modelPersonalities.claude.systemMessage,
        messages: [{ role: 'user', content: fullPrompt }],
      });
      return r.content[0].text.trim();
    }

    if (modeloIA === 'gemini' && gemini) {
      const model = gemini.getGenerativeModel({
        model: 'gemini-1.5-pro',
        generationConfig: { maxOutputTokens: 200, temperature: 0.0 }
      });
      const full = `${modelPersonalities.gemini.systemMessage}\n\n${fullPrompt}`;
      const result = await model.generateContent([full]);
      const response = await result.response;
      return response.text().trim();
    }

    if (modeloIA === 'deepseek' && deepseek) {
      const r = await deepseek.chat.completions.create({
        model: 'deepseek-chat',
        temperature: 0.0,
        max_tokens: 300,
        messages: [
          { role: 'system', content: modelPersonalities.deepseek.systemMessage },
          { role: 'user', content: fullPrompt },
        ],
      });
      return r.choices[0].message.content.trim();
    }

    // Se nenhum modelo configurado, retornar undefined para fallback
    return undefined;
  } catch (err) {
    console.error('‚ùå Erro ao pedir decis√£o de busca ao modelo:', err.message);
    return undefined;
  }
}

// ====== FUN√á√ÉO QUE PROCESSA COM O MODELO (RESPOSTA FINAL) ======
async function processWithAI(promptToProcess, modeloIA) {
  try {
    const personality = modelPersonalities[modeloIA] || modelPersonalities.openai;
    if (modeloIA === 'openai' && openai) {
      const r = await openai.chat.completions.create({
        model: 'gpt-4o',
        temperature: personality.temperature,
        max_tokens: personality.maxTokens,
        messages: [
          { role: 'system', content: personality.systemMessage },
          { role: 'user', content: promptToProcess }
        ],
      });
      let resposta = r.choices[0].message.content.trim();
      if (r.choices[0].finish_reason === 'length') {
        resposta += "\n\n‚ö†Ô∏è Resposta atingiu limite de tokens";
      }
      return resposta;
    }

    if (modeloIA === 'claude' && claude) {
      const r = await claude.messages.create({
        model: 'claude-sonnet-4-20250514',
        max_tokens: personality.maxTokens,
        temperature: personality.temperature,
        system: personality.systemMessage,
        messages: [{ role: 'user', content: promptToProcess }],
      });
      let resposta = r.content[0].text.trim();
      if (r.stop_reason === 'max_tokens') {
        resposta += "\n\n‚ö†Ô∏è Resposta atingiu limite de tokens";
      }
      return resposta;
    }

    if (modeloIA === 'gemini' && gemini) {
      const model = gemini.getGenerativeModel({
        model: 'gemini-1.5-pro',
        generationConfig: {
          maxOutputTokens: personality.maxTokens,
          temperature: personality.temperature,
        }
      });
      const fullPrompt = `${personality.systemMessage}\n\nUsu√°rio: ${promptToProcess}`;
      const result = await model.generateContent([fullPrompt]);
      const response = await result.response;
      return response.text().trim();
    }

    if (modeloIA === 'deepseek' && deepseek) {
      const r = await deepseek.chat.completions.create({
        model: 'deepseek-chat',
        temperature: personality.temperature,
        max_tokens: personality.maxTokens,
        messages: [
          { role: 'system', content: personality.systemMessage },
          { role: 'user', content: promptToProcess },
        ],
      });
      let resposta = r.choices[0].message.content.trim();
      if (r.choices[0].finish_reason === 'length') {
        resposta += "\n\n‚ö†Ô∏è Resposta atingiu limite de tokens";
      }
      return resposta;
    }

    throw new Error(`Modelo ${modeloIA} n√£o dispon√≠vel`);
  } catch (error) {
    console.error(`‚ùå Erro no ${modeloIA}:`, error.message);
    throw error;
  }
}

// ====== ROTAS B√ÅSICAS ======
app.get('/', (_req, res) => {
  res.status(200).send({ message: 'Hello from chat lateral!' });
});

app.get('/api/status', (_req, res) => {
  res.json({
    message: 'API funcionando',
    modelos: {
      openai: !!openai,
      claude: !!claude,
      gemini: !!gemini,
      deepseek: !!deepseek
    },
    searchEnabled: !!process.env.SEARCH_API_KEY,
    instructionsLoaded: searchInstructions.instructions.length || 0
  });
});


const indicationsPath = path.join(__dirname, 'indications.json');
let indications = [];
try {
  const raw = fs.readFileSync(indicationsPath, 'utf8');
  const parsed = JSON.parse(raw);
  indications = Array.isArray(parsed.profiles) ? parsed.profiles : (parsed?.profiles || []);
  console.log(`‚úÖ indications.json carregado (${indications.length} perfis)`);
} catch (e) {
  console.warn('‚ö†Ô∏è indications.json n√£o encontrado ou inv√°lido ‚Äî fallback ativo.', e.message);
}

// Fun√ß√£o de recomenda√ß√£o 100% baseada no JSON
function normalizeText(s) {
  return (s || '')
    .toString()
    .toLowerCase()
    .normalize('NFD')                      // separa diacr√≠ticos
    .replace(/[\u0300-\u036f]/g, '')       // remove diacr√≠ticos
    .replace(/[^a-z0-9\s]/g, ' ')          // remove pontua√ß√£o mantendo espa√ßos
    .replace(/\s+/g, ' ')                  // reduz m√∫ltiplos espa√ßos
    .trim();
}

function recommendFromIndications(userPrompt) {
  const raw = (userPrompt || '').toString();
  if (!raw || !Array.isArray(indications) || indications.length === 0) return null;

  const promptNorm = normalizeText(raw);
  const promptWords = promptNorm.split(' ').filter(Boolean);
  const promptFull = ` ${promptNorm} `; // para buscar substrings com espa√ßos

  // calcular scores
  const scored = indications.map((profile, idx) => {
    let score = 0;
    const profileTextParts = [
      (profile.tags || []).join(' '),
      (profile.specialties || []).join(' '),
      (profile.strengths || []).join(' '),
      (profile.ideal_use_cases || []).join(' ')
    ];
    const profileFullRaw = profileTextParts.join(' ');
    const profileFull = ` ${normalizeText(profileFullRaw)} `;

    // 1) Matches exatos em tags/specialties (alto peso)
    for (const tag of (profile.tags || [])) {
      const t = ` ${normalizeText(tag)} `;
      if (profileFull.includes(t) && promptFull.includes(t.trim())) {
        score += 4; // match exato
      }
    }
    for (const spec of (profile.specialties || [])) {
      const s = normalizeText(spec);
      if (s && promptFull.includes(` ${s} `)) score += 3;
    }

    // 2) Matches por tokens (robusto contra varia√ß√µes)
    for (const w of promptWords) {
      if (!w) continue;
      if (profileFull.includes(` ${w} `)) {
        score += 1.5;
      } else if (profileFull.includes(w)) {
        score += 0.7;
      }
    }

    // 3) refor√ßo se qualquer ideal_use_case aparece como substring
    for (const usecase of (profile.ideal_use_cases || [])) {
      const u = normalizeText(usecase);
      if (u && promptFull.includes(u)) score += 1.2;
    }

    // 4) pequeno boost por quantidade de descri√ß√£o (n√£o decisivo)
    score += Math.min(((profile.strengths||[]).length + (profile.specialties||[]).length) / 30, 0.5);

    return { profile, score, idx };
  });

  // ordenar e escolher
  scored.sort((a,b) => b.score - a.score || a.idx - b.idx);
  const top = scored[0];

  // Se nenhum score positivo, fallback heur√≠stico
  if (!top || top.score <= 0) {
    const taskType = detectTaskType(userPrompt);
    const suggestion = getAISuggestion(taskType);
    const fallbackProfile = indications.find(p => p.id === suggestion.best) || (indications[0] || {});
    return {
      best: fallbackProfile.id || suggestion.best,
      reason: (suggestion.reason || (fallbackProfile.strengths?.[0] || 'Adequado para a necessidade')).slice(0, 160),
      source: 'heuristic'
    };
  }

  const chosen = top.profile;

  // construir motivo em tom "copy", curto
  const candidate = (chosen.strengths && chosen.strengths[0]) ? chosen.strengths[0] : (chosen.specialties && chosen.specialties[0]) || '';
  const reason = normalizeText(candidate).split(' ').slice(0, 12).join(' ');
  console.log(`üîé recommendFromIndications -> chosen: ${chosen.id} | score: ${top.score.toFixed(2)} | prompt="${promptNorm}"`);
  // para debug detalhado (opcional)
  // console.log(scored.map(s=>`${s.profile.id}:${s.score.toFixed(2)}`).join(' | '));

  return { best: chosen.id, reason: reason || 'Adequado para a necessidade descrita', source: 'indications.json' };
}

app.post('/api/suggest-ai-model', async (req, res) => {
  try {
    const { prompt } = req.body || {};
    if (!prompt || !prompt.trim()) return res.status(400).json({ error: 'Prompt √© obrigat√≥rio.' });

    if (indications && indications.length > 0) {
      const rec = recommendFromIndications(prompt);
      if (rec) return res.json({ best: rec.best, reason: rec.reason, source: rec.source, fallback: false });
    }

    // fallback para heur√≠stica se json ausente
    const taskType = detectTaskType(prompt);
    const suggestion = getAISuggestion(taskType);
    return res.json({ best: suggestion.best, reason: suggestion.reason, source: 'heuristic', fallback: true });

  } catch (err) {
    console.error('Erro /api/suggest-ai-model (indications.json):', err?.message || err);
    return res.status(500).json({ error: 'Erro interno ao decidir IA.' });
  }
});


// ====== ROTA PRINCIPAL: DECIS√ÉO VIA MODELO + BUSCA (quando necess√°rio) ======
app.post('/api/rota', async (req, res) => {
  try {
    const { prompt: rawPrompt, modeloIA = 'openai', contextoMemoria = null } = req.body || {};
    const prompt = (rawPrompt || '').trim();

    // ADICIONAR: identificar usu√°rio e rastrear a mensagem recebida
    const userId = (req.body && req.body.userId) ? req.body.userId : 'anonymous';
    // Conta UMA mensagem do usu√°rio (cada chamada /api/rota = 1 mensagem enviada)
    trackMessageSent(userId);


    console.log(`üìù Prompt: "${prompt}" | Modelo: ${modeloIA}${contextoMemoria ? ' | Com contexto de mem√≥ria' : ''}`);

    if (!prompt) return res.status(400).json({ error: 'Prompt √© obrigat√≥rio.' });

    const taskType = detectTaskType(prompt);
    const suggestion = getAISuggestion(taskType);

    // Verifica modelo dispon√≠vel
    const modelosDisponiveis = { openai, claude, gemini, deepseek };
    if (!modelosDisponiveis[modeloIA]) {
      return res.status(400).json({
        error: `Modelo ${modeloIA} n√£o configurado. Verifique as chaves de API.`
      });
    }

    // MODIFICA√á√ÉO: Se h√° contexto de mem√≥ria, use-o no prompt para decis√£o
    let promptParaDecisao = prompt;
    if (contextoMemoria) {
      promptParaDecisao = `${contextoMemoria}\n${prompt}`;
    }

    // 1) Pedir ao modelo para decidir se precisa buscar (sa√≠da estrita)
    const decisionRaw = await askModelForSearchDecision(promptParaDecisao, modeloIA);
    console.log('üßæ Decis√£o bruta do modelo:', decisionRaw);

    // Extrair tag
    let tag = null;
    if (decisionRaw) {
      const tagMatch = decisionRaw.match(/#(NO_SEARCH|MAYBE_SEARCH|NEEDS_SEARCH)/i);
      if (tagMatch) tag = `#${tagMatch[1].toUpperCase()}`;
    }

    // Se o modelo n√£o respondeu corretamente, assume MAYBE para ser seguro
    if (!tag) {
      console.log('‚ö†Ô∏è Tag n√£o encontrada na resposta do modelo. Assumindo #MAYBE_SEARCH por seguran√ßa.');
      tag = '#MAYBE_SEARCH';
    }

    // Extrair SEARCH_QUERY se presente
    let searchQuery = null;
    if (decisionRaw) {
      const queryMatch = decisionRaw.match(/SEARCH_QUERY:\s*(.+)/i);
      if (queryMatch) searchQuery = queryMatch[1].trim();
    }

    console.log(`üîñ Decis√£o final: ${tag} ${searchQuery ? `| Query: ${searchQuery}` : ''}`);

    let finalAnswer = '';
    let searchUsed = false;
    let searchResults = null;

    // Se tag indica que precisa de busca, realiza busca externa
    if (tag === '#MAYBE_SEARCH' || tag === '#NEEDS_SEARCH') {
      // Gerar query fallback simples se modelo n√£o forneceu
      if (!searchQuery) {
        const terms = prompt.split(/\s+/).slice(0, 6).join(' ');
        searchQuery = `${terms}`;
      }

      // Tentar buscar
      const results = await performWebSearch(searchQuery);
      searchResults = results;
      if (results) {
        const contextualPrompt = formatSearchResults(results, prompt);
        // MODIFICA√á√ÉO: Incluir contexto de mem√≥ria tamb√©m na resposta final
        let promptWithContext = `${contextualPrompt}\n\n**PERGUNTA ORIGINAL:** ${prompt}`;
        if (contextoMemoria) {
          promptWithContext = `${contextoMemoria}\n\n${contextualPrompt}\n\n**PERGUNTA ORIGINAL:** ${prompt}`;
        }
        finalAnswer = await processWithAI(promptWithContext, modeloIA);
        searchUsed = true;
      } else {
        // Fallback: sem acesso √† web, pe√ßa ao modelo responder com base no conhecimento interno
        console.log('‚ö†Ô∏è Busca falhou ou indispon√≠vel ‚Äî usando fallback sem web.');
        let fallbackPrompt = `Sem acesso √† web. Responda com base no seu conhecimento mais atualizado:\n\n${prompt}`;
        if (contextoMemoria) {
          fallbackPrompt = `${contextoMemoria}\n\nSem acesso √† web. Responda com base no seu conhecimento mais atualizado:\n\n${prompt}`;
        }
        finalAnswer = await processWithAI(fallbackPrompt, modeloIA);
        searchUsed = false;
      }
    } else {
      // #NO_SEARCH -> responder direto
      let directPrompt = prompt;
      if (contextoMemoria) {
        directPrompt = `${contextoMemoria}\n\n${prompt}`;
      }
      finalAnswer = await processWithAI(directPrompt, modeloIA);
      searchUsed = false;
    }

    // Opcional: salvar chat (pode ser comentado se n√£o quiser persistir)
    try {
      if (req.body.saveChat) {
        const chatToSave = new Chat({
          userId: req.body.userId || 'anonymous',
          prompt,
          response: finalAnswer,
          createdAt: req.body.createdAt || new Date().toISOString()
        });
        await chatToSave.save();
        console.log('üì• Chat salvo via /api/rota (saveChat=true)');
      }
    } catch (e) {
      console.log('‚ö†Ô∏è N√£o foi poss√≠vel salvar chat:', e.message);
    }
    
    return res.json({
      bot: finalAnswer,
      taskType,
      suggestedAI: suggestion.best,
      decisionTag: tag,
      searchUsed,
      searchQuery: searchUsed ? searchQuery : null,
      searchResultsSummary: searchUsed && searchResults ? (searchResults.organic?.slice(0,3).map(r=>({ title: r.title, link: r.link })) ) : null,
      contextoMemoriaUsado: !!contextoMemoria // Indica se contexto foi usado
    });

  } catch (err) {
    console.error('üí• Erro:', err.message);
    res.status(500).json({
      error: 'Erro ao processar requisi√ß√£o.',
      details: process.env.NODE_ENV === 'development' ? err.message : 'Erro interno'
    });
  }
});

app.post('/api/refine-prompt', async (req, res) => {
  try {
    const { prompt: rawPrompt, modeloIA = 'deepseek' } = req.body || {};
    const prompt = (rawPrompt || '').trim();

    if (!prompt) return res.status(400).json({ error: 'prompt √© obrigat√≥rio.' });
    if (prompt.length > 4000) return res.status(400).json({ error: 'Prompt muito longo. M√°x 4000 caracteres.' });

    // Apenas N√≠vel 2 (estruturado) ‚Äî sempre usado
    const systemInstruction = `VOC√ä √â UMA IA ESPECIALISTA EM APRIMORAR PROMPTS PARA OUTRAS IAs.
ATEN√á√ÉO: N√ÉO EXECUTE, N√ÉO RESPONDA E N√ÉO RESOLVA A TAREFA DO USU√ÅRIO. Sua √∫nica tarefa √© transformar o prompt do usu√°rio em um prompt mais claro, completo e acion√°vel para outra IA.

REGRAS OBRIGAT√ìRIAS:
1) N√ÉO fa√ßa a tarefa nem forne√ßa a resposta ‚Äî apenas reescreva o prompt.
2) Preserve a inten√ß√£o e os fatos originais; N√ÉO invente informa√ß√µes.
3) Melhore clareza, estrutura e contexto (objetivo, p√∫blico, formato de sa√≠da, restri√ß√µes).
4) Sugira tom e formato desejado quando necess√°rio (ex.: "em t√≥picos", "resumo executivo", "exemplos").
5) Seja conciso, profissional e direto ‚Äî priorize instru√ß√µes que facilitem a execu√ß√£o pela IA destino.
6) Sa√≠da EXATA: apenas o prompt refinado.
7) Avalie se a tarefa exige detalhamento; se sim, forne√ßa um prompt refinado detalhado; se n√£o, n√£o envie refinamento, exemplos de tarefas que n√£o precisa de detalhamento (Estas s√£o tarefas simples, diretas, que qualquer IA consegue executar com um prompt curto: "Resuma um texto de 200 palavras." "Crie uma sugest√£o de t√≠tulo para um artigo." "Gere uma lista de 10 hashtags para Instagram." "Formate esse texto abaixo em t√≥picos....." "Traduza o par√°grafo para ingl√™s....."). Estas tarefas s√£o mais complexas, envolvem m√∫ltiplos elementos ou contexto, e exigem prompts refinados: "Monte um plano de marketing digital para o lan√ßamento de um produto." "Crie um roteiro completo para um v√≠deo institucional de 5 minutos." "Fa√ßa uma an√°lise detalhada dos concorrentes no setor de tecnologia.""Escreva um relat√≥rio financeiro trimestral com gr√°ficos e insights." "Desenvolva um plano de treinamento de equipe de vendas para 3 meses."
8) Ao receber uma solicita√ß√£o que envolva depurar, corrigir, analisar ou otimizar algum conte√∫do, o refinamento deve se concentrar exclusivamente na parte da instru√ß√£o do usu√°rio (por exemplo: "analise meu c√≥digo e veja se tem erro"). O conte√∫do propriamente dito (como o c√≥digo, texto ou relat√≥rio) deve ser mantido intacto, sem qualquer modifica√ß√£o ou reescrita, e inclu√≠do entre aspas no prompt refinado, assim: {"conte√∫do original"}. O objetivo √© preservar o material enviado e garantir que o refinamento foque apenas na tarefa solicitada.

Exemplos de como refinar (apenas para refer√™ncia):

prompt: "escreva uma descri√ß√£o da vaga de emprego"
prompt refinado: "Escreva uma descri√ß√£o completa para vaga, incluindo responsabilidades, qualifica√ß√µes, compet√™ncias, benef√≠cios e instru√ß√µes de candidatura."

prompt: "corrija erros no meu c√≥digo abaixo (.......)"
prompt refinado: "Depure o c√≥digo JavaScript, corrija erros e otimize fun√ß√µes mantendo compatibilidade com navegadores modernos."


`;

    // verificar provedores
    const providers = {
      openai: !!openai,
      claude: !!claude,
      gemini: !!gemini,
      deepseek: !!deepseek
    };
    if (!providers.openai && !providers.claude && !providers.gemini && !providers.deepseek) {
      return res.status(503).json({ error: 'Nenhum provedor de IA configurado no servidor.' });
    }

    // Ordem de tentativas: modelo solicitado primeiro, depois fallbacks
    const order = [modeloIA, 'deepseek', 'openai', 'claude', 'gemini']
      .filter((v, i, a) => v && a.indexOf(v) === i);

    // Fun√ß√£o que chama um provedor para refinamento (temperatura fixa para consist√™ncia)
    async function callProviderForRefine(providerKey, systemMsg, userPrompt) {
      try {
        const temp = 0.2;

        if (providerKey === 'openai' && openai) {
          const r = await openai.chat.completions.create({
            model: 'gpt-4o',
            temperature: temp,
            max_tokens: 600,
            messages: [
              { role: 'system', content: systemMsg },
              { role: 'user', content: userPrompt }
            ]
          });
          return r?.choices?.[0]?.message?.content?.trim() || '';
        }

        if (providerKey === 'claude' && claude) {
          const r = await claude.messages.create({
            model: 'claude-sonnet-4-20250514',
            max_tokens: 600,
            temperature: temp,
            system: systemMsg,
            messages: [{ role: 'user', content: userPrompt }]
          });
          return (r?.content?.[0]?.text || r?.text || '').trim();
        }

        if (providerKey === 'gemini' && gemini) {
          const model = gemini.getGenerativeModel({
            model: 'gemini-1.5-pro',
            generationConfig: { maxOutputTokens: 600, temperature: temp }
          });
          const full = `${systemMsg}\n\n${userPrompt}`;
          const result = await model.generateContent([full]);
          const response = await result.response;
          return (response?.text || '').trim();
        }

        if (providerKey === 'deepseek' && deepseek) {
          const r = await deepseek.chat.completions.create({
            model: 'deepseek-chat',
            temperature: temp,
            max_tokens: 600,
            messages: [
              { role: 'system', content: systemMsg },
              { role: 'user', content: userPrompt }
            ]
          });
          return r?.choices?.[0]?.message?.content?.trim() || '';
        }

        return '';
      } catch (err) {
        console.warn(`‚ö†Ô∏è Erro ao chamar provedor ${providerKey}:`, err.message || err);
        return '';
      }
    }

    // Tentar na ordem at√© obter um refinamento n√£o vazio
    let refined = '';
    let usedProvider = null;
    for (const p of order) {
      if (!providers[p]) continue;
      refined = await callProviderForRefine(p, systemInstruction, prompt);
      if (refined && refined.length > 0) {
        usedProvider = p;
        break;
      }
    }

    if (!refined) {
      return res.status(502).json({ error: 'N√£o foi poss√≠vel gerar prompt refinado com os provedores dispon√≠veis.' });
    }

    console.log(`üîß /api/refine-prompt -> provider: ${usedProvider} | level: 2 | chars: ${refined.length}`);

    return res.json({ refinedPrompt: refined, provider: usedProvider, level: 2 });

  } catch (err) {
    console.error('Erro /api/refine-prompt:', err?.message || err);
    res.status(500).json({ error: 'Erro ao refinar prompt.' });
  }
});

app.post('/api/gerar-resumo', async (req, res) => {
  try {
    const { mensagens, limite = 200, modeloIA = 'openai' } = req.body || {};
    
    if (!mensagens || !Array.isArray(mensagens) || mensagens.length === 0) {
      return res.status(400).json({ error: 'Mensagens s√£o obrigat√≥rias para gerar resumo.' });
    }

    // Prepara o conte√∫do das mensagens para resumir
    const conteudo = mensagens.map((msg, index) => {
      const papel = msg.role === 'user' ? 'Usu√°rio' : 'IA';
      return `${papel}: ${msg.text}`;
    }).join('\n\n');

    // Prompt espec√≠fico para gerar resumo de mem√≥ria
    const promptResumo = `Voc√™ √© um especialista em resumir conversas de forma inteligente e concisa.

TAREFA: Resuma a conversa abaixo mantendo as informa√ß√µes mais importantes e o contexto essencial.
LIMITE: ${limite} palavras m√°ximo
ESTILO: Conciso, claro e informativo

INSTRU√á√ïES:
- Mantenha os pontos principais discutidos
- Preserve o contexto importante para futuras intera√ß√µes
- Use linguagem clara e objetiva
- Foque no que √© relevante para dar continuidade √† conversa

CONVERSA:
${conteudo}

RESUMO:`;

    // Usa o modelo especificado para gerar o resumo
    const resumo = await processWithAI(promptResumo, modeloIA);
    
    console.log(`üß† Resumo gerado - Modelo: ${modeloIA} | Mensagens: ${mensagens.length} | Chars: ${resumo.length}`);
    
    res.json({ 
      resumo: resumo,
      modelo: modeloIA,
      mensagensProcessadas: mensagens.length,
      caracteres: resumo.length
    });
    
  } catch (err) {
    console.error('‚ùå Erro ao gerar resumo:', err.message);
    res.status(500).json({ 
      error: 'Erro ao gerar resumo das mensagens.',
      details: process.env.NODE_ENV === 'development' ? err.message : 'Erro interno'
    });
  }
});

// ====== ROTAS DE CHAT ======
app.post('/api/chats', async (req, res) => {
  try {
    const { userId, prompt, response, createdAt } = req.body || {};
    if (!userId || !prompt || !response) return res.status(400).json({ error: 'Dados obrigat√≥rios faltando.' });

    // ADICIONAR: contar como mensagem enviada
    trackMessageSent(userId);

    const newChat = new Chat({
      userId,
      prompt,
      response,
      createdAt: createdAt || new Date().toISOString(),
    });
    await newChat.save();
    res.json({ message: 'Chat salvo!', chat: newChat });
  } catch (err) {
    res.status(500).json({ error: 'Erro ao salvar chat.' });
  }
});

app.get('/api/chats/:userId', async (req, res) => {
  try {
    const chats = await Chat.find({ userId: req.params.userId }).sort({ createdAt: -1 });
    res.json(chats);
  } catch (err) {
    res.status(500).json({ error: 'Erro ao buscar chats.' });
  }
});

app.delete('/api/chats/:chatId', async (req, res) => {
  try {
    await Chat.findByIdAndDelete(req.params.chatId);
    res.json({ success: true });
  } catch (err) {
    res.status(500).json({ error: 'Erro ao excluir chat.' });
  }
});

// ====== ROTAS INFORMATIVAS ======
app.get('/api/specializations', (_req, res) => {
  const specializations = {
    openai: { name: "ChatGPT", role: "Generalista Vers√°til", available: !!openai },
    claude: { name: "Claude", role: "An√°lises Profundas", available: !!claude },
    gemini: { name: "Gemini", role: "Produtividade", available: !!gemini },
    deepseek: { name: "DeepSeek", role: "Dados & Programa√ß√£o", available: !!deepseek }
  };
  res.json({ specializations });
});

// ====== SERVIDOR ======
const PORT = process.env.PORT || 5001;
app.listen(PORT, () => {
  console.log(`üöÄ Chat PROFISSIONAL com DECIS√ÉO EMBUTIDA - http://localhost:${PORT}`);
  console.log(`üöÄ Chat PROFISSIONAL com DECIS√ÉO EMBUTIDA - http://localhost:5001`);
  console.log(`üß† Sistema de busca: ATIVO para TODAS as IAs`);
  console.log(`üéØ IAs dispon√≠veis:`);
  console.log(`   ChatGPT: ${openai ? '‚úÖ Generalista + Busca Inteligente' : '‚ùå'}`);
  console.log(`   Claude: ${claude ? '‚úÖ An√°lises + Busca Inteligente' : '‚ùå'}`);
  console.log(`   Gemini: ${gemini ? '‚úÖ Produtividade + Busca Inteligente' : '‚ùå'}`);
  console.log(`   DeepSeek: ${deepseek ? '‚úÖ Dados + Busca Inteligente' : '‚ùå'}`);
  console.log(`üîç Sistema de busca otimizado:`);
  console.log(`   üîë API de busca: ${process.env.SEARCH_API_KEY ? '‚úÖ Configurada' : '‚ùå N√£o configurada'}`);
  console.log(`‚ú® Sistema ultra inteligente - busca apenas quando necess√°rio!`);
});